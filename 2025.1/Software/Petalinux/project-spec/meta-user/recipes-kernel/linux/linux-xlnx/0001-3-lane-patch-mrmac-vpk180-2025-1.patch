From 9e6850abd9e78631e2e229aa87ea381f59631984 Mon Sep 17 00:00:00 2001
From: Ajaya Dahal <ajayad@amd.com>
Date: Mon, 17 Nov 2025 11:28:37 -0800
Subject: [PATCH] 3-lane-patch-mrmac-vpk180-2025-1

---
 drivers/net/ethernet/xilinx/xilinx_axienet.h  |   9 ++
 .../net/ethernet/xilinx/xilinx_axienet_main.c | 124 ++++++++++++++++--
 2 files changed, 121 insertions(+), 12 deletions(-)

diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet.h b/drivers/net/ethernet/xilinx/xilinx_axienet.h
index f9a7e79293aa..83fe5346a960 100644
--- a/drivers/net/ethernet/xilinx/xilinx_axienet.h
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet.h
@@ -83,6 +83,11 @@
 				 XAE_OPTION_FLOW_CONTROL | \
 				 XAE_OPTION_RXEN)
 
+
+#define DELAY_OF_ONE_SEC		1000000
+#define MUX_10G_MASK 			0xF
+#define MUX_25G_MASK 			0x0
+
 /* Axi DMA Register definitions */
 
 #define XAXIDMA_TX_CR_OFFSET	0x00000000 /* Channel control */
@@ -649,6 +654,8 @@ enum temac_stat {
 #define XAE_MAX_PKT_LEN		8192
 
 /* MRMAC Register Definitions */
+#define MRMAC_GT_PLL_DONE_LANE_0_1_2_MASK	0x77
+
 /* Configuration Registers */
 #define MRMAC_REV_OFFSET		0x00000000
 #define MRMAC_RESET_OFFSET		0x00000004
@@ -860,6 +867,7 @@ struct axidma_bd {
  * @tx_skb:	  Transmit skb address
  * @tx_desc_mapping: Tx Descriptor DMA mapping type.
  * @page:	page buffer to access the data passed by GRO packet
+ * @axi_mux: Mux is used for speed switching
  */
 struct aximcdma_bd {
 	u32 next;	/* Physical address of next buffer descriptor */
@@ -1045,6 +1053,7 @@ struct axienet_local {
 	resource_size_t regs_start;
 	void __iomem *regs;
 	void __iomem *mcdma_regs;
+	void __iomem *axi_mux;  /* mux is used for speed switching */
 
 	u64_stats_t tx_packets;
 	u64_stats_t tx_bytes;
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet_main.c b/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
index 2b69f35a92fb..087fcc0942a0 100644
--- a/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
@@ -96,12 +96,33 @@ EXPORT_SYMBOL(mrmac_gt_pll);
 void __iomem *mrmac_gt_ctrl;
 EXPORT_SYMBOL(mrmac_gt_ctrl);
 
+void __iomem *axi_mux_gpio;
+EXPORT_SYMBOL(axi_mux_gpio);
+
 int mrmac_pll_reg;
 EXPORT_SYMBOL(mrmac_pll_reg);
 
 int mrmac_pll_rst;
 EXPORT_SYMBOL(mrmac_pll_rst);
 
+int axi_mux_reg;
+EXPORT_SYMBOL(axi_mux_reg);
+
+static inline void axienet_reset_mux(struct axienet_local *lp)
+{
+	u32 val;
+
+	val = ioread32(lp->axi_mux);
+
+	if (lp->max_speed == SPEED_25000) {
+		val &= ~(1 << lp->gt_lane);
+	iowrite32(val, lp->axi_mux);
+	} else {
+		val |= 1 << lp->gt_lane;
+		iowrite32(val, lp->axi_mux);
+	}
+}
+
 static void axienet_rx_submit_desc(struct net_device *ndev);
 
 /* Option table for setting up Axi Ethernet hardware options */
@@ -843,18 +864,24 @@ static inline int axienet_mrmac_gt_reset(struct net_device *ndev)
 
 	if (mrmac_pll_rst == 0) {
 		for (i = 0; i < MRMAC_MAX_GT_LANES; i++) {
-			iowrite32(MRMAC_GT_RST_ALL_MASK, (lp->gt_ctrl +
-				  (MRMAC_GT_LANE_OFFSET * i) +
-				  MRMAC_GT_CTRL_OFFSET));
-			mdelay(DELAY_1MS);
-			iowrite32(0, (lp->gt_ctrl + (MRMAC_GT_LANE_OFFSET * i) +
-				      MRMAC_GT_CTRL_OFFSET));
+			if((i==0) || (i==2)){
+				iowrite32(MRMAC_GT_RST_ALL_MASK, (lp->gt_ctrl +
+					(MRMAC_GT_LANE_OFFSET * i) +
+					MRMAC_GT_CTRL_OFFSET));
+				
+				
+				mdelay(DELAY_1MS*300);
+				iowrite32(0, (lp->gt_ctrl + (MRMAC_GT_LANE_OFFSET * i) +
+						MRMAC_GT_CTRL_OFFSET));
+				//break;
+			}
+
 		}
 
 		/* Wait for PLL lock with timeout */
 		err = readl_poll_timeout(lp->gt_pll + MRMAC_GT_PLL_STS_OFFSET,
-					 val, (val & MRMAC_GT_PLL_DONE_MASK),
-					 10, DELAY_OF_ONE_MILLISEC * 100);
+					 val, ((val & MRMAC_GT_PLL_DONE_LANE_0_1_2_MASK) == MRMAC_GT_PLL_DONE_LANE_0_1_2_MASK),
+					 10, DELAY_OF_ONE_MILLISEC*1000);
 		if (err) {
 			netdev_err(ndev, "MRMAC PLL lock not complete! Cross-check the MAC ref clock configuration\n");
 			return -ENODEV;
@@ -1026,6 +1053,8 @@ static int axienet_device_reset(struct net_device *ndev)
 
 	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
 		/* Reset MRMAC */
+		if (axi_mux_reg)
+			axienet_reset_mux(lp);
 		axienet_mrmac_reset(lp);
 	}
 
@@ -3543,7 +3572,13 @@ axienet_ethtools_get_coalesce(struct net_device *ndev,
 	for_each_rx_dma_queue(lp, i) {
 		q = lp->dq[i];
 
-		regval = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		#ifdef CONFIG_AXIENET_HAS_MCDMA
+			regval = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      XMCDMA_RX_OFFSET);
+		#else
+			regval = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		#endif
+
 		ecoalesce->rx_max_coalesced_frames +=
 						(regval & XAXIDMA_COALESCE_MASK)
 						     >> XAXIDMA_COALESCE_SHIFT;
@@ -3551,7 +3586,11 @@ axienet_ethtools_get_coalesce(struct net_device *ndev,
 	}
 	for_each_tx_dma_queue(lp, i) {
 		q = lp->dq[i];
-		regval = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		#ifdef CONFIG_AXIENET_HAS_MCDMA
+			regval = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		#else
+			regval = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		#endif
 		ecoalesce->tx_max_coalesced_frames +=
 						(regval & XAXIDMA_COALESCE_MASK)
 						     >> XAXIDMA_COALESCE_SHIFT;
@@ -3665,6 +3704,34 @@ static void axienet_ethtools_get_ethtool_stats(struct net_device *dev,
 	} while (read_seqcount_retry(&lp->hw_stats_seqcount, start));
 }
 
+static int axienet_change_speed(struct net_device *dev, const struct ethtool_link_ksettings *cmd)
+{
+	struct axienet_local *lp = netdev_priv(dev);
+	int ret = 0;
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		lp->max_speed = cmd->base.speed;
+		return 0;
+	} else {
+
+		return phy_ethtool_set_link_ksettings(dev, cmd);
+	}
+}
+
+static int axienet_get_speed(struct net_device *dev,  struct ethtool_link_ksettings *cmd)
+{
+	struct axienet_local *lp = netdev_priv(dev);
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		cmd->base.speed = lp->max_speed;
+		return 0;
+	} else {
+
+		return phy_ethtool_get_link_ksettings(dev, cmd);
+	}
+} 
+
+
 static const char axienet_ethtool_stats_strings[][ETH_GSTRING_LEN] = {
 	"Received bytes",
 	"Transmitted bytes",
@@ -3994,8 +4061,8 @@ static const struct ethtool_ops axienet_ethtool_ops = {
 #ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
 	.get_ts_info    = axienet_ethtools_get_ts_info,
 #endif
-	.get_link_ksettings = axienet_ethtools_get_link_ksettings,
-	.set_link_ksettings = axienet_ethtools_set_link_ksettings,
+	.get_link_ksettings = axienet_get_speed,
+	.set_link_ksettings = axienet_change_speed,
 	.nway_reset	= axienet_ethtools_nway_reset,
 	.get_ethtool_stats = axienet_ethtools_get_ethtool_stats,
 	.get_strings    = axienet_ethtools_get_strings,
@@ -4847,6 +4914,7 @@ static int axienet_probe(struct platform_device *pdev)
 	struct resource txtsres, rxtsres;
 #endif
 	u16 num_queues = XAE_MAX_QUEUES;
+	struct resource axi_mux;
 
 	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-queues",
 				   &num_queues);
@@ -5151,6 +5219,38 @@ static int axienet_probe(struct platform_device *pdev)
 			goto cleanup_clk;
 		}
 		dev_info(&pdev->dev, "GT lane: %d\n", lp->gt_lane);
+
+		if(axi_mux_reg) {
+				lp->axi_mux = axi_mux_gpio;
+				dev_info(&pdev->dev, "AXI MUX REG: %d\n", lp->axi_mux);
+		} else {
+			np = of_parse_phandle(pdev->dev.of_node,
+					"xlnx,axi_mux", 0);
+			if (IS_ERR(np)) {
+				dev_warn(&pdev->dev,
+						"couldn't find axi_mux switching not supported\n");
+			} else {
+
+				ret = of_address_to_resource(np, 0, &axi_mux);
+				dev_info(&pdev->dev, "AXI MUX from DT: %x\n\n", &axi_mux);
+				if (ret) {
+					dev_warn(&pdev->dev,
+							"unable to get axi_mux resource\n");
+				}
+
+				lp->axi_mux = devm_ioremap_resource(&pdev->dev,
+						&axi_mux);
+				if (IS_ERR(lp->axi_mux)) {
+					dev_warn(&pdev->dev,
+							"couldn't map axi_mux regs\n");
+				} else {
+					axi_mux_gpio = lp->axi_mux;
+					axi_mux_reg = 1;
+				}
+			}
+
+		}
+
 	} else if (lp->axienet_config->mactype == XAXIENET_DCMAC) {
 		lp->gds_gt_ctrl = devm_gpiod_get_array(&pdev->dev,
 						       "gt_ctrl",
-- 
2.25.1

